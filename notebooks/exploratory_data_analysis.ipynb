{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Commerce Competitive Intelligence - Exploratory Data Analysis\n",
    "\n",
    "**Author:** Jack Massey  \n",
    "**Date:** 2026-01-25  \n",
    "**Purpose:** Comprehensive EDA of 374 days of hourly web scraping data from a beauty/haircare e-commerce reseller\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Overview\n",
    "- **Rows:** 1,724,927 observations\n",
    "- **Time Period:** December 9, 2024 \u2192 December 19, 2025 (374 days)\n",
    "- **Brands:** 24 beauty/haircare brands\n",
    "- **Products:** 239 unique products with 280 variants\n",
    "- **Frequency:** Hourly snapshots (8,947 timestamps)\n",
    "\n",
    "This notebook consolidates the exploratory data analysis from scripts 01-03, providing interactive visualizations and insights into the competitive landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "\n",
    "print(\"\u2713 Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "DATA_PATH = Path('../data/processed/cleaned_data.parquet')\n",
    "\n",
    "print(f\"Loading data from: {DATA_PATH}\")\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "print(f\"\\n\u2713 Loaded {len(df):,} rows \u00d7 {len(df.columns)} columns\")\n",
    "print(f\"\u2713 Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nDate Range: {df['date'].min()} \u2192 {df['date'].max()}\")\n",
    "print(f\"Duration: {(df['date'].max() - df['date'].min()).days} days\")\n",
    "\n",
    "print(f\"\\nUnique Counts:\")\n",
    "print(f\"  - Brands: {df['brand'].nunique()}\")\n",
    "print(f\"  - Products: {df['product_id'].nunique()}\")\n",
    "print(f\"  - Variants: {df['offer_offer_id'].nunique()}\")\n",
    "print(f\"  - Timestamps: {df['date'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Initial Exploration (Column Analysis)\n",
    "\n",
    "Understanding each column's characteristics: data type, unique values, missing data, and sample values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-level analysis\n",
    "def analyze_columns(df):\n",
    "    \"\"\"\n",
    "    Analyze each column to understand:\n",
    "    - Data type\n",
    "    - Number of unique values\n",
    "    - Number of missing values\n",
    "    - Sample values\n",
    "    \"\"\"\n",
    "    analysis = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        n_unique = df[col].nunique()\n",
    "        n_missing = df[col].isna().sum()\n",
    "        pct_missing = (n_missing / len(df)) * 100\n",
    "        dtype = df[col].dtype\n",
    "        \n",
    "        # Get sample values (first 3 non-null)\n",
    "        sample_values = df[col].dropna().head(3).tolist()\n",
    "        \n",
    "        analysis.append({\n",
    "            'column': col,\n",
    "            'dtype': str(dtype),\n",
    "            'n_unique': n_unique,\n",
    "            'n_missing': n_missing,\n",
    "            'pct_missing': round(pct_missing, 2),\n",
    "            'sample_values': str(sample_values)[:100]  # Truncate for display\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(analysis)\n",
    "\n",
    "# Run analysis\n",
    "column_analysis = analyze_columns(df)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"COLUMN ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "column_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    missing_pct = (missing_data / len(df)) * 100\n",
    "    missing_pct.plot(kind='barh', ax=ax, color='coral')\n",
    "    ax.set_xlabel('Missing Data (%)')\n",
    "    ax.set_title('Missing Data by Column')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(missing_pct):\n",
    "        ax.text(v + 0.1, i, f'{v:.2f}%', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\u2713 No missing values detected (except description field)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problematic Columns Identification\n",
    "\n",
    "Looking for columns that might be problematic:\n",
    "- Constant columns (only 1 unique value)\n",
    "- Empty columns (all nulls)\n",
    "- High missing rate (>50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_problematic_columns(df, analysis_df):\n",
    "    \"\"\"\n",
    "    Identify columns that might be useless or problematic.\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    for idx, row in analysis_df.iterrows():\n",
    "        col = row['column']\n",
    "        \n",
    "        # Constant columns\n",
    "        if row['n_unique'] == 1:\n",
    "            issues.append({\n",
    "                'column': col,\n",
    "                'issue': 'CONSTANT',\n",
    "                'detail': f\"Only 1 unique value\",\n",
    "                'recommendation': 'REMOVE - No variance'\n",
    "            })\n",
    "        \n",
    "        # All missing\n",
    "        elif row['pct_missing'] == 100:\n",
    "            issues.append({\n",
    "                'column': col,\n",
    "                'issue': 'EMPTY',\n",
    "                'detail': 'All values are null',\n",
    "                'recommendation': 'REMOVE - No data'\n",
    "            })\n",
    "        \n",
    "        # High missing rate (>50%)\n",
    "        elif row['pct_missing'] > 50:\n",
    "            issues.append({\n",
    "                'column': col,\n",
    "                'issue': 'HIGH_MISSING',\n",
    "                'detail': f\"{row['pct_missing']}% missing\",\n",
    "                'recommendation': 'INVESTIGATE - Might be optional field'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(issues) if issues else None\n",
    "\n",
    "# Identify issues\n",
    "issues_df = identify_problematic_columns(df, column_analysis)\n",
    "\n",
    "if issues_df is not None and len(issues_df) > 0:\n",
    "    print(\"=\" * 100)\n",
    "    print(\"PROBLEMATIC COLUMNS IDENTIFIED\")\n",
    "    print(\"=\" * 100)\n",
    "    display(issues_df)\n",
    "else:\n",
    "    print(\"\u2713 No obvious problematic columns found in cleaned dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection (Numeric Columns)\n",
    "\n",
    "Detecting potential data errors using the 3-sigma rule (values >3 standard deviations from mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_potential_errors(df):\n",
    "    \"\"\"\n",
    "    Detect potential data entry errors in numeric columns.\n",
    "    Look for outliers (>3 standard deviations from mean).\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    outliers_summary = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        \n",
    "        if std == 0:  # Skip columns with no variance\n",
    "            continue\n",
    "        \n",
    "        # Find outliers (>3 std from mean)\n",
    "        outliers = df[(df[col] > mean + 3*std) | (df[col] < mean - 3*std)]\n",
    "        \n",
    "        if len(outliers) > 0:\n",
    "            outliers_summary.append({\n",
    "                'column': col,\n",
    "                'n_outliers': len(outliers),\n",
    "                'pct_outliers': round((len(outliers) / len(df)) * 100, 2),\n",
    "                'mean': round(mean, 2),\n",
    "                'std': round(std, 2),\n",
    "                'outlier_range': f\"<{round(mean - 3*std, 2)} or >{round(mean + 3*std, 2)}\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(outliers_summary) if outliers_summary else None\n",
    "\n",
    "# Detect outliers\n",
    "outliers_df = detect_potential_errors(df)\n",
    "\n",
    "if outliers_df is not None:\n",
    "    print(\"=\" * 100)\n",
    "    print(\"POTENTIAL DATA ERRORS (Outliers)\")\n",
    "    print(\"=\" * 100)\n",
    "    display(outliers_df)\n",
    "else:\n",
    "    print(\"\u2713 No significant outliers detected in numeric columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of numeric columns\n",
    "numeric_cols = ['offer_price', 'stock']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Price distribution\n",
    "axes[0].hist(df['offer_price'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df['offer_price'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: \u20ac{df['offer_price'].mean():.2f}\")\n",
    "axes[0].axvline(df['offer_price'].median(), color='green', linestyle='--', linewidth=2, label=f\"Median: \u20ac{df['offer_price'].median():.2f}\")\n",
    "axes[0].set_xlabel('Price (EUR)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Price Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Stock distribution\n",
    "axes[1].hist(df['stock'], bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(df['stock'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {df['stock'].mean():.1f} units\")\n",
    "axes[1].axvline(df['stock'].median(), color='green', linestyle='--', linewidth=2, label=f\"Median: {df['stock'].median():.1f} units\")\n",
    "axes[1].set_xlabel('Stock (units)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Stock Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPrice Stats: Min=\u20ac{df['offer_price'].min():.2f}, Max=\u20ac{df['offer_price'].max():.2f}, Mean=\u20ac{df['offer_price'].mean():.2f}\")\n",
    "print(f\"Stock Stats: Min={df['stock'].min()}, Max={df['stock'].max()}, Mean={df['stock'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Structure Understanding\n",
    "\n",
    "Deep dive into what each column represents and how data is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 10 rows\n",
    "print(\"=\" * 100)\n",
    "print(\"FIRST 10 ROWS - Understanding Data Structure\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column-by-Column Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"COLUMN-BY-COLUMN EXAMINATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# 1. Product ID\n",
    "print(\"\\n1. PRODUCT_ID - URL slug identifier\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"   Total unique: {df['product_id'].nunique()}\")\n",
    "print(f\"   Sample values:\")\n",
    "for val in df['product_id'].head(5):\n",
    "    print(f\"      \u2022 {val}\")\n",
    "print(f\"   Pattern: URL-friendly slugs (lowercase, hyphenated product names)\")\n",
    "\n",
    "# 2. Title\n",
    "print(\"\\n2. TITLE - Product names\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"   Total unique: {df['title'].nunique()}\")\n",
    "print(f\"   Sample values:\")\n",
    "for val in df['title'].head(5):\n",
    "    print(f\"      \u2022 {val}\")\n",
    "print(f\"   Pattern: Human-readable product names (French, uppercase)\")\n",
    "\n",
    "# 3. Description\n",
    "print(\"\\n3. DESCRIPTION - Product details\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"   Total unique: {df['description'].nunique()}\")\n",
    "print(f\"   Missing: {df['description'].isna().sum()} ({df['description'].isna().sum()/len(df)*100:.2f}%)\")\n",
    "if df['description'].notna().sum() > 0:\n",
    "    first_desc = df['description'].dropna().iloc[0]\n",
    "    print(f\"   Sample value (first non-null):\")\n",
    "    print(f\"      {first_desc[:200]}...\")\n",
    "\n",
    "# 4. Brand\n",
    "print(\"\\n4. BRAND - All brands\")\n",
    "print(\"-\" * 100)\n",
    "brands = sorted(df['brand'].unique())\n",
    "print(f\"   Total unique: {len(brands)}\")\n",
    "print(f\"   All brands:\")\n",
    "for i, brand in enumerate(brands, 1):\n",
    "    print(f\"      {i:2d}. {brand}\")\n",
    "\n",
    "# 5. Offer Title\n",
    "print(\"\\n5. OFFER_TITLE - Product variants\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"   Total unique: {df['offer_title'].nunique()}\")\n",
    "print(f\"   Most common values:\")\n",
    "top_offers = df['offer_title'].value_counts().head(10)\n",
    "for offer, count in top_offers.items():\n",
    "    print(f\"      \u2022 '{offer}': {count:,} occurrences\")\n",
    "\n",
    "# 6. Offer Offer ID\n",
    "print(\"\\n6. OFFER_OFFER_ID - Unique variant identifier\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"   Total unique: {df['offer_offer_id'].nunique()}\")\n",
    "print(f\"   Sample values: {df['offer_offer_id'].head(5).tolist()}\")\n",
    "print(f\"   Pattern: Shopify variant ID (13-digit integer)\")\n",
    "\n",
    "# 7. Offer Price\n",
    "print(\"\\n7. OFFER_PRICE - Price in EUR\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"   Range: \u20ac{df['offer_price'].min():.2f} - \u20ac{df['offer_price'].max():.2f}\")\n",
    "print(f\"   Mean: \u20ac{df['offer_price'].mean():.2f}\")\n",
    "print(f\"   Median: \u20ac{df['offer_price'].median():.2f}\")\n",
    "\n",
    "# 8. Offer In Stock\n",
    "print(\"\\n8. OFFER_IN_STOCK - Boolean availability flag\")\n",
    "print(\"-\" * 100)\n",
    "stock_status = df['offer_in_stock'].value_counts()\n",
    "for status, count in stock_status.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"      \u2022 {status}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# 9. Stock\n",
    "print(\"\\n9. STOCK - Quantity available\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"   Range: {df['stock'].min()} - {df['stock'].max()} units\")\n",
    "print(f\"   Mean: {df['stock'].mean():.2f} units\")\n",
    "print(f\"   Median: {df['stock'].median():.2f} units\")\n",
    "\n",
    "# 10. Date\n",
    "print(\"\\n10. DATE - Timestamp of scrape\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"   Data type: {df['date'].dtype}\")\n",
    "print(f\"   Unique timestamps: {df['date'].nunique():,}\")\n",
    "print(f\"   First timestamp: {df['date'].min()}\")\n",
    "print(f\"   Last timestamp: {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Relationships & Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"DATA RELATIONSHIPS & HIERARCHY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Products per brand\n",
    "print(\"\\n1. How many products does each brand have?\")\n",
    "print(\"-\" * 100)\n",
    "products_per_brand = df.groupby('brand')['product_id'].nunique().sort_values(ascending=False)\n",
    "for brand, count in products_per_brand.items():\n",
    "    print(f\"   {brand:30s}: {count:3d} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize products per brand\n",
    "fig = px.bar(products_per_brand.reset_index(), \n",
    "             x='brand', \n",
    "             y='product_id',\n",
    "             title='Number of Products per Brand',\n",
    "             labels={'product_id': 'Number of Products', 'brand': 'Brand'},\n",
    "             color='product_id',\n",
    "             color_continuous_scale='Blues')\n",
    "fig.update_layout(xaxis_tickangle=-45, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offers per product (variants)\n",
    "print(\"\\n2. Do products have multiple variants (offers)?\")\n",
    "print(\"-\" * 100)\n",
    "offers_per_product = df.groupby('product_id')['offer_offer_id'].nunique()\n",
    "print(f\"   Products with 1 variant: {(offers_per_product == 1).sum()}\")\n",
    "print(f\"   Products with 2+ variants: {(offers_per_product > 1).sum()}\")\n",
    "print(f\"   Max variants per product: {offers_per_product.max()}\")\n",
    "print(f\"\\n   Example - Products with multiple variants:\")\n",
    "multi_variant_products = offers_per_product[offers_per_product > 1].head(10)\n",
    "for product, n_variants in multi_variant_products.items():\n",
    "    product_title = df[df['product_id'] == product]['title'].iloc[0]\n",
    "    print(f\"      \u2022 '{product_title}': {n_variants} variants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal coverage per brand\n",
    "print(\"\\n3. Do all brands have the same temporal coverage?\")\n",
    "print(\"-\" * 100)\n",
    "brand_dates = df.groupby('brand')['date'].agg(['min', 'max', 'nunique']).reset_index()\n",
    "brand_dates.columns = ['Brand', 'First_Date', 'Last_Date', 'N_Timestamps']\n",
    "brand_dates = brand_dates.sort_values('N_Timestamps', ascending=False)\n",
    "brand_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal coverage\n",
    "fig = px.bar(brand_dates, \n",
    "             x='Brand', \n",
    "             y='N_Timestamps',\n",
    "             title='Temporal Coverage by Brand (Number of Timestamps)',\n",
    "             labels={'N_Timestamps': 'Number of Timestamps', 'Brand': 'Brand'},\n",
    "             color='N_Timestamps',\n",
    "             color_continuous_scale='Viridis')\n",
    "fig.update_layout(xaxis_tickangle=-45, height=500)\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Insight: {(brand_dates['N_Timestamps'] == brand_dates['N_Timestamps'].max()).sum()} brands have complete coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Detailed Analysis\n",
    "\n",
    "Deep dive into specific aspects: brands, pricing, stock levels, and variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brand Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 120)\n",
    "print(\"BRAND ANALYSIS\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "brands = df['brand'].unique()\n",
    "print(f\"\\nTotal unique brand names: {len(brands)}\\n\")\n",
    "\n",
    "print(\"All brand names with statistics:\")\n",
    "print(\"-\" * 120)\n",
    "brand_stats = []\n",
    "for brand in sorted(brands):\n",
    "    count = (df['brand'] == brand).sum()\n",
    "    n_products = df[df['brand'] == brand]['product_id'].nunique()\n",
    "    brand_stats.append({\n",
    "        'Brand': brand,\n",
    "        'Total_Rows': count,\n",
    "        'N_Products': n_products\n",
    "    })\n",
    "\n",
    "brand_stats_df = pd.DataFrame(brand_stats)\n",
    "brand_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brand duplicate investigation\n",
    "print(\"\\n=\" * 120)\n",
    "print(\"BRAND DUPLICATE INVESTIGATION\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Look for similar brand names\n",
    "potential_duplicates = []\n",
    "\n",
    "for i, brand1 in enumerate(brands):\n",
    "    for brand2 in brands[i+1:]:\n",
    "        # Check for similarity (case-insensitive, whitespace-normalized)\n",
    "        norm1 = brand1.strip().upper().replace(' ', '')\n",
    "        norm2 = brand2.strip().upper().replace(' ', '')\n",
    "        \n",
    "        if norm1 == norm2:\n",
    "            potential_duplicates.append((brand1, brand2))\n",
    "\n",
    "if potential_duplicates:\n",
    "    print(\"\\n DUPLICATE BRANDS DETECTED:\")\n",
    "    for brand1, brand2 in potential_duplicates:\n",
    "        count1 = (df['brand'] == brand1).sum()\n",
    "        count2 = (df['brand'] == brand2).sum()\n",
    "        products1 = df[df['brand'] == brand1]['product_id'].nunique()\n",
    "        products2 = df[df['brand'] == brand2]['product_id'].nunique()\n",
    "        \n",
    "        print(f\"\\n   Brand 1: '{brand1}' ({count1:,} rows, {products1} products)\")\n",
    "        print(f\"   Brand 2: '{brand2}' ({count2:,} rows, {products2} products)\")\n",
    "        print(f\"   RECOMMENDATION: These should be merged under a single name\")\n",
    "else:\n",
    "    print(\"\\n\u2713 No obvious duplicates found based on normalized names\")\n",
    "    print(\"  (Note: Data cleaning script already merged 'AUXPORTESDUNATUREL' duplicates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product-brand consistency check\n",
    "print(\"\\n=\" * 120)\n",
    "print(\"PRODUCT-BRAND CONSISTENCY CHECK\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Count brands per product\n",
    "product_brands = df.groupby('product_id')['brand'].nunique()\n",
    "multi_brand_products = product_brands[product_brands > 1]\n",
    "\n",
    "if len(multi_brand_products) > 0:\n",
    "    print(f\"\\n   \u26a0\ufe0f  ISSUE FOUND: {len(multi_brand_products)} products appear under multiple brands!\")\n",
    "    print(f\"\\n   Examples:\")\n",
    "    for product in multi_brand_products.head(5).index:\n",
    "        brands_list = df[df['product_id'] == product]['brand'].unique()\n",
    "        print(f\"      \u2022 Product '{product}' appears under: {', '.join(brands_list)}\")\n",
    "    print(f\"\\n   RECOMMENDATION: Investigate and standardize brand assignment\")\n",
    "else:\n",
    "    print(\"\\n   \u2713 All products consistently assigned to single brand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution by brand\n",
    "fig = px.box(df, \n",
    "             x='brand', \n",
    "             y='offer_price',\n",
    "             title='Price Distribution by Brand',\n",
    "             labels={'offer_price': 'Price (EUR)', 'brand': 'Brand'},\n",
    "             color='brand')\n",
    "fig.update_layout(xaxis_tickangle=-45, height=600, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price ranges\n",
    "print(\"\\nPRICE DISTRIBUTION BY RANGE\")\n",
    "print(\"-\" * 80)\n",
    "price_ranges = [0, 10, 20, 30, 50, 100, 200]\n",
    "for i in range(len(price_ranges)-1):\n",
    "    count = ((df['offer_price'] >= price_ranges[i]) & (df['offer_price'] < price_ranges[i+1])).sum()\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"   \u20ac{price_ranges[i]:3d}-\u20ac{price_ranges[i+1]:3d}: {count:8,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Over max range\n",
    "count_over = (df['offer_price'] >= price_ranges[-1]).sum()\n",
    "pct_over = count_over / len(df) * 100\n",
    "print(f\"   \u20ac{price_ranges[-1]:3d}+     : {count_over:8,} ({pct_over:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock distribution by brand\n",
    "fig = px.box(df, \n",
    "             x='brand', \n",
    "             y='stock',\n",
    "             title='Stock Distribution by Brand',\n",
    "             labels={'stock': 'Stock (units)', 'brand': 'Brand'},\n",
    "             color='brand')\n",
    "fig.update_layout(xaxis_tickangle=-45, height=600, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock ranges\n",
    "print(\"\\nSTOCK DISTRIBUTION BY RANGE\")\n",
    "print(\"-\" * 80)\n",
    "stock_ranges = [0, 10, 25, 50, 100, 200, 800]\n",
    "for i in range(len(stock_ranges)-1):\n",
    "    count = ((df['stock'] >= stock_ranges[i]) & (df['stock'] < stock_ranges[i+1])).sum()\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"   {stock_ranges[i]:3d}-{stock_ranges[i+1]:3d} units: {count:8,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Over max range\n",
    "count_over = (df['stock'] >= stock_ranges[-1]).sum()\n",
    "pct_over = count_over / len(df) * 100\n",
    "print(f\"   {stock_ranges[-1]:3d}+ units     : {count_over:8,} ({pct_over:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offer title distribution\n",
    "print(\"\\nOFFER VARIANT ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "offer_counts = df['offer_title'].value_counts()\n",
    "print(f\"\\nTotal unique offer variants: {len(offer_counts)}\")\n",
    "print(f\"\\nTop 15 most common variants:\")\n",
    "for i, (offer, count) in enumerate(offer_counts.head(15).items(), 1):\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"   {i:2d}. '{offer}': {count:8,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Count default vs custom variants\n",
    "default_count = (df['offer_title'] == 'Default Title').sum()\n",
    "custom_count = len(df) - default_count\n",
    "print(f\"\\nVariant Type Breakdown:\")\n",
    "print(f\"   Default Title: {default_count:,} ({default_count/len(df)*100:.2f}%)\")\n",
    "print(f\"   Custom Variants: {custom_count:,} ({custom_count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize variant distribution\n",
    "variant_data = pd.DataFrame({\n",
    "    'Variant Type': ['Default Title', 'Custom Variants'],\n",
    "    'Count': [default_count, custom_count]\n",
    "})\n",
    "\n",
    "fig = px.pie(variant_data, \n",
    "             values='Count', \n",
    "             names='Variant Type',\n",
    "             title='Product Variants: Default vs Custom',\n",
    "             color_discrete_sequence=['#636EFA', '#EF553B'])\n",
    "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Format Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDATE FORMAT CONSISTENCY CHECK\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Sample dates throughout dataset\n",
    "sample_indices = np.linspace(0, len(df)-1, 10, dtype=int)\n",
    "sample_dates = df.iloc[sample_indices]['date']\n",
    "\n",
    "print(\"\\n   Sample dates from throughout dataset:\")\n",
    "for i, date in enumerate(sample_dates, 1):\n",
    "    print(f\"      {i}. {date}\")\n",
    "\n",
    "print(f\"\\n   \u2713 Data type: {df['date'].dtype} (datetime64)\")\n",
    "print(f\"   \u2713 Format is consistent and properly converted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Insights Summary\n",
    "\n",
    "Key findings from the exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "#### Data Quality\n",
    "- **Clean Dataset**: All problematic columns (constant values, empty fields, URLs) have been removed in the cleaning process\n",
    "- **Missing Data**: Only 5.66% of descriptions are missing, which is acceptable for an optional field\n",
    "- **Brand Consistency**: All products are consistently assigned to a single brand (no conflicts)\n",
    "- **Date Format**: All timestamps properly converted to datetime64 format\n",
    "\n",
    "#### Brand Portfolio\n",
    "- **24 unique brands** in the dataset (down from 25 after merging duplicates)\n",
    "- **Uneven distribution**: Top brands like LES SECRETS DE LOLY (27 products), CUT BY FRED (23), and CENTIFOLIA (20)\n",
    "- **Temporal coverage**: Most brands have complete coverage across all 8,947 timestamps, indicating stable tracking\n",
    "\n",
    "#### Product & Variants\n",
    "- **239 unique products** with **280 variants** total\n",
    "- **218 products** have only 1 variant (\"Default Title\")\n",
    "- **21 products** have multiple variants (colors, sizes, subscription periods)\n",
    "- **Maximum 11 variants** per product (likely gift cards or subscription products)\n",
    "- **89% of records** use \"Default Title\" (single-variant products dominate)\n",
    "\n",
    "#### Pricing Insights\n",
    "- **Price range**: \u20ac0 - \u20ac180, with mean of \u20ac22.12 and median slightly lower\n",
    "- **Most products (47%)** priced in the \u20ac10-\u20ac20 range (accessible price point)\n",
    "- **Premium segment**: Only 3.37% of products above \u20ac50\n",
    "- **Price distribution** varies significantly by brand, reflecting different positioning strategies\n",
    "\n",
    "#### Inventory Insights\n",
    "- **Stock availability**: 96.17% of records show products in stock (healthy inventory)\n",
    "- **Stock levels**: Range from 0 to 741 units, mean of 41 units\n",
    "- **Low stockout rate**: Only 3.83% out of stock, indicating good inventory management\n",
    "- **Stock distribution** shows most products maintain moderate inventory (25-100 units)\n",
    "\n",
    "#### Potential Data Quality Issues\n",
    "- **Price outliers**: 1.84% of records show unusual pricing (likely data entry errors or special promotions)\n",
    "- **Stock outliers**: 1.96% of records with unusual stock levels (possibly bulk restocks or errors)\n",
    "- These outliers are minimal and don't significantly impact overall analysis\n",
    "\n",
    "The cleaned dataset is ready for advanced analytics and visualization in the Streamlit dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}